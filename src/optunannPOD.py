# -*- coding: utf-8 -*-
"""OptunaNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12YTL-BNbXzlPZrWj99maFceivAlqwPve
"""

# -*- coding: utf-8 -*-
"""SubSpaceJSOptuna.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I9vXOXmCfmNA6dAk1ZJagTkIx6YJVkS7
"""

# -*- coding: utf-8 -*-
"""SubSpaceJS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13TKGc6sVjhS5Ok9MTliNmm8aGU8X4xX8
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf

from sklearn.metrics import accuracy_score, precision_score, recall_score, r2_score, mean_squared_error
from sklearn.model_selection import train_test_split, KFold
from tensorflow.keras import layers, losses
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Model

import random
import sys
import time
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.backend as keras_backend

from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import AveragePooling2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import concatenate
from sklearn.feature_selection import mutual_info_regression
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize
from operator import add
from numpy import asarray
import math
import scipy
import seaborn as sns
#import networkx as nx
import numpy as np
import optuna as optuna
import json


##################### Model Creation

def create_model(
                 neurons_input = 1, num_of_layers_1=1,
                  lr=0.01, moment = 0.5, actF="relu", lossF="mean_squared_error"):

  model = Sequential()
  for i in range(num_of_layers_1):
    model.add(Dense(units=neurons_input, activation=actF))
    model.add(BatchNormalization(momentum=moment))
  ##final layer
  model.add(Dense(units=1))#, kernel_regularizer=tf.keras.regularizers.l1(0.01), activity_regularizer=tf.keras.regularizers.l2(0.01)))
  opt1 = tf.keras.optimizers.Nadam(learning_rate=lr)
  model.compile(loss=lossF, optimizer=opt1, metrics=['mse','mae','mape'])
  return model
#####################K fold Generation
def kfoldValidation(model, X, Y, fold, epochS, batchS):
  kfold = KFold(n_splits= fold, shuffle=False)
  mse_per_fold = []
  mae_per_fold = []
  mape_per_fold = []
  fold_no = 1
  testmodel = model
  for train, test in kfold.split(X, Y):
    rowx = tf.gather(X, train)
    rowy = tf.gather(Y, train)
    history = testmodel.fit(rowx, rowy,
                batch_size=batchS,
                epochs=epochS,
                verbose=0)
    tx = tf.gather(X, test)
    ty = tf.gather(Y, test)

    scores = testmodel.evaluate(tx, ty, verbose=0)
    #print(f'Score for fold {fold_no}: {testmodel.metrics_names[0]} of {scores[0]}; {testmodel.metrics_names[1]} of {scores[1]}; {testmodel.metrics_names[2]} of {scores[2]}; {testmodel.metrics_names[3]} of {scores[3]};')

    mse_per_fold.append(scores[1])
    mae_per_fold.append(scores[2])
    mape_per_fold.append(scores[3])
    fold_no += 1
  return testmodel, mse_per_fold, mae_per_fold, mape_per_fold

class Objective(object):
  def __init__(self, originalP, original_labelsP, turn, cp_path, numFolds):
    # Hold this implementation specific arguments as the fields of the class.
    self.org = originalP
    self.labels = original_labelsP
    self.nol = turn
    self.savingPath = cp_path
    self.NumFolds = numFolds
  #def callback(self, study, trial):
    #if hvd.rank() == 0:
    #if study.best_trial.number == trial.number:
      #self.model.save_weights(self.savingPath+f"r-Trial-{trial.number}-model")

  def __call__(self, trial):
    num_layers = trial.suggest_int('num_layers', 1, 10, 2)
    #neuron = trial.suggest_int('neuron', low=10, high=1000, step=50)
    neuron = trial.suggest_categorical("neuron", [10, 50, 100, 200, 300, 500, 600, 800, 900, 1000] )
    batch_size = trial.suggest_int('batch_size', low = 50, high = 300, step=50)
    momentum = trial.suggest_float('Momentum', low=0.4, high=1.0, step=0.1)
    #lr2 = trial.suggest_loguniform('lr2', 1e-7, 1e-1)
    lr2 = trial.suggest_categorical("lr2", [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0])
    self.model  = create_model(neurons_input = neuron, num_of_layers_1=num_layers, lr = lr2, moment=momentum, actF="relu", lossF="mean_squared_error")
    losses = []
    total_loss = 0
    ls = 0
    self.model, mse, mae, mape = kfoldValidation(self.model, self.org, self.labels, self.NumFolds, self.nol, batch_size)
    ls = math.sqrt(np.mean(mse))
    losses.append(ls)
    #trial.report(ls)
    #if trial.should_prune():
    #    raise optuna.exceptions.TrialPruned()
    #model.save_weights(self.savingPath+f"Trial-{trial.number}-model")
    return ls

def finder(originalP, original_labelsP, epochs, checkpoint_path, num_of_trials, fold, stname, storageName):
  #tf.debugging.set_log_device_placement(True)
  with tf.device('/GPU:0'):
    loaded_study = optuna.load_study(study_name=stname, storage=storageName)
    obj = Objective(originalP, original_labelsP, epochs, checkpoint_path, fold)
    #loaded_study.optimize(obj, n_trials= num_of_trials, callbacks=[obj.callback], gc_after_trial=True)
    loaded_study.optimize(obj, n_trials= num_of_trials, gc_after_trial=True)
    trial = loaded_study.best_trial
  return trial
                            
